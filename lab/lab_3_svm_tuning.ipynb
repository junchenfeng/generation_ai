{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Feature Engineering and SVM Model\n",
    "\n",
    "在本实验中，我们将聚焦数据特征工程\n",
    "1. 数据预处理的基本步骤\n",
    "2. 特征工程的常用技巧\n",
    "3. 特征预筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Say hello to the old friend\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/loan_data.csv').head(2000)  # 为了避免后面计算性能降低太多\n",
    "X = data.drop('loan_status', axis=1)\n",
    "y = data['loan_status']\n",
    "\n",
    "# 原则上，你不应该知道test数据集的分布\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理\n",
    "\n",
    "### 2.1 处理缺失值\n",
    "\n",
    "大部分现实世界的数据集都有大量的缺失值。处理缺失值的不同方法可能会对于模型性能产生显著的影响。处理缺失值的方法\n",
    "\n",
    "    - 删除：最简单但是最糟糕的做法。不仅导致数据量减少，而且实际上改变了数据集分布，增加了模型过拟合风险，降低泛化能力\n",
    "    - 填充：\n",
    "        - 均值填充/中位数填充/众数填充：简单易行，比删除数据好，但是也会影响模型效果。特别是缺失比例较高时\n",
    "        - 模型填充：比较复杂。可能化腐朽为神奇，也可能garbage in garbage out。\n",
    "\n",
    "在我们的lab和我推荐的大部分数据集里，我们都不需要考虑缺失值处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 One-hot Encoding\n",
    "\n",
    "数据集里的离散变量需要被提前识别和转化为一系列0/1变量。这个过程叫One hot encoding。\n",
    "\n",
    "在One Hot Encoding中，一般都会drop一个类别，否则会因为变量间共线性导致模型无法收敛。这个对于线性模型是致命的，但是对于其他非线形模型并不重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 数值和分类特征列表\n",
    "\n",
    "categorical_features = [\n",
    "    'person_gender', 'person_education', 'person_home_ownership',\n",
    "    'loan_intent', 'previous_loan_defaults_on_file'\n",
    "]\n",
    "\n",
    "transformers = [\n",
    "    ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO*: 看看现在categorical_features里有哪些变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 特征缩放比较\n",
    "\n",
    "下面我们来展示一个标准的特征缩放过程，即正态化。这避免某些特征的尺度对于其他特征的影响。比如把某个省的GDP和它的上年增长率放在一起，GDP的尺度会对于模型产生显著影响。\n",
    "\n",
    "NOTE: copy的作用是保留原始数据，在反复运行中不会污染原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "numerical_features = [\n",
    "    'person_age', 'person_income', 'person_emp_exp', 'loan_amnt',\n",
    "    'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length',\n",
    "    'credit_score'\n",
    "]\n",
    "transformers_copy = copy.deepcopy(transformers)\n",
    "transformers_copy.append(('num', StandardScaler(), numerical_features))\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers_copy)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', Perceptron(random_state=42))\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO*：绘制原始数据的mean和std，以及标准化后的mean和std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在完整训练集上训练最终模型\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 在测试集上评估模型性能\n",
    "y_pred = pipeline.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"使用Pipeline的分类报告:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*HW* 为什么predict时没有做特征处理？what happens？pipeline起了什么作用？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**正态化不是唯一的缩放方法**\n",
    "\n",
    "原则上processors的编排也是可以用cross validation来进行筛选的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "# 用cross validation选择最佳的scaler\n",
    "cv_results = {}\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    transformers_copy = copy.deepcopy(transformers)\n",
    "    transformers_copy.append(('num', scaler, numerical_features))\n",
    "\n",
    "    # 创建pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', ColumnTransformer(transformers_copy)),\n",
    "        ('classifier', Perceptron(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # 使用5折交叉验证评估模型\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # 存储结果\n",
    "    cv_results[name] = scores.mean()\n",
    "\n",
    "# 打印每个scaler的平均准确率\n",
    "for name, score in cv_results.items():\n",
    "    print(f\"{name} 的平均交叉验证准确率: {score:.6f}\")\n",
    "\n",
    "# 找出最佳的scaler\n",
    "best_scaler = max(cv_results.items(), key=lambda x: x[1])[0]\n",
    "print(f\"\\n最佳scaler是: {best_scaler}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特征工程\n",
    "\n",
    "最常见也一般最有效的特征工程方法是特征交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# 使用最佳的scaler创建预处理器\n",
    "transformers_copy = copy.deepcopy(transformers)\n",
    "transformers_copy.append(('num',  StandardScaler(), numerical_features))\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers_copy)\n",
    "\n",
    "# 对训练集和测试集进行转换\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "# 创建多项式特征\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True) \n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "\n",
    "print(\"原始特征数量:\", X_train.shape[1])\n",
    "print(\"交互特征后的数量:\", X_train_poly.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW** 注意上面用了interactions only这个参数。试试去掉这个参数会发生什么？为什么会出现这个现象？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将这个流程添加到pipeline中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建完整的pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=2, interaction_only=True)),\n",
    "    ('classifier', Perceptron(random_state=42))\n",
    "])\n",
    "\n",
    "# 在训练集上拟合pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# 打印分类报告\n",
    "print(\"使用Pipeline的分类报告:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO*: 比较两个pipeline的结果，这说明了什么？让大模型帮助你写一个能被其他人读懂的“学术描述”。这是论文写作的核心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM & Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 SVM参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，让我们调整C参数。回忆lab 2的参数调整方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_c_list = [0.1, 1, 10, 100]\n",
    "param_grid = {\n",
    "    'classifier__C': param_c_list,  # 添加classifier__前缀\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=2, interaction_only=True)),\n",
    "    ('classifier', SVC(random_state=42, kernel='linear'))\n",
    "])\n",
    "\n",
    "# 创建网格搜索对象\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,  # 使用pipeline作为estimator\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# 在训练集上进行网格搜索\n",
    "grid_search.fit(X_train, y_train)  \n",
    "\n",
    "print(\"最佳参数:\", grid_search.best_params_)\n",
    "print(\"最佳得分:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO* 把n_jobs设置为4，比较运行时间；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取最佳模型\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# 获取预处理后的特征名称\n",
    "feature_names = (\n",
    "    # 获取分类特征的名称\n",
    "    best_model.named_steps['preprocessor']\n",
    "    .named_transformers_['cat']\n",
    "    .get_feature_names_out([\n",
    "        'person_gender', 'person_education', 'person_home_ownership',\n",
    "        'loan_intent', 'previous_loan_defaults_on_file'\n",
    "    ]).tolist() +\n",
    "    # 获取数值特征的名称\n",
    "    [\n",
    "        'person_age', 'person_income', 'person_emp_exp',\n",
    "        'loan_amnt', 'loan_int_rate', 'loan_percent_income',\n",
    "        'cb_person_cred_hist_length', 'credit_score'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 获取多项式特征的名称\n",
    "poly_features = best_model.named_steps['poly'].get_feature_names_out(feature_names)\n",
    "\n",
    "# 获取SVC的系数（仅适用于线性核）\n",
    "if best_model.named_steps['classifier'].kernel == 'linear':\n",
    "    coefficients = best_model.named_steps['classifier'].coef_[0]\n",
    "    \n",
    "    # 创建特征名称和系数的DataFrame\n",
    "    coef_df = pd.DataFrame({\n",
    "        '特征': poly_features,\n",
    "        '系数': coefficients\n",
    "    })\n",
    "    \n",
    "    # 显示系数为0的特征\n",
    "    print(\"系数为0的特征：\")\n",
    "    print(coef_df[coef_df['系数'] == 0])\n",
    "    \n",
    "    # 显示绝对值最大的前10个系数\n",
    "    print(\"\\n影响最大的前10个特征：\")\n",
    "    print(coef_df.reindex(coef_df['系数'].abs().sort_values(ascending=False).index)[:10])\n",
    "else:\n",
    "    print(\"非线性核函数不能直接解释特征重要性\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 核函数\n",
    "核函数是另一种形式的“特征”工程。它将特征从欧几里得空间映射到其他（高维）空间，从而改变了“距离”的物理含义。\n",
    "\n",
    "虽然早期核函数的应用都有一定的现实原因，但是随着kernel trick的提出，核函数逐渐成为了一种数学技巧，而不再需要现实原因。\n",
    "\n",
    "广义地说，核函数描述了数据生成过程（data generating process, DGP）的一部分，policy的目标是尽可能拟合DGP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义新的参数网格,增加kernel的搜索\n",
    "param_grid = {\n",
    "    'classifier__C': param_c_list,\n",
    "    'classifier__kernel': ['linear', 'rbf'],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=2, interaction_only=True)),\n",
    "    ('classifier', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# 使用网格搜索\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 打印最佳参数\n",
    "print(\"最佳参数:\", grid_search.best_params_)\n",
    "\n",
    "# 在测试集上评估\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(\"\\n测试集准确率:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n分类报告:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 预训练阶段的特征选择\n",
    "\n",
    "事实上即使我们不去拟合复杂模型，也可以对于特征进行有效选择，这样可以降低计算成本。\n",
    "\n",
    "*TODO/HW* 尝试使用logistic regression 对特征进行预选择，将精炼后的特征再放进SVM中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair program with LLM here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.总结\n",
    "\n",
    "这个lab结束后，我们掌握了5种工具来改善模型最后的表现：\n",
    "- 编码（onehot encoding）和缩放（Scaler）\n",
    "- 特征工程【我们只讲了多项式这一种技巧，事实上还有其他技巧】\n",
    "- 模型\n",
    "- 模型参数的Grid Search\n",
    "- 评估指标\n",
    "\n",
    "还有2类工具我们没有讲但是在实际分类任务中非常重要\n",
    "- 缺失值处理\n",
    "- 样本不均衡处理 （例如正样本只有2%要怎么办？）\n",
    "\n",
    "\n",
    "因此，ML的最终调优是 算法技术，算力和分析经验共同作用的结果。经过这么多年发展，现在约束ML调优的基本上就是算力和分析经验驱动的特征工程了\n",
    "\n",
    "\n",
    "就在Data Scientist成为Data Monkey的时候，出现了深度学习革命。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
