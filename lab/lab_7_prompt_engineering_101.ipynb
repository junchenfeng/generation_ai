{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: 提示词工程(Prompt Engineering 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.chatGPT是如何实现的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # 敏感信息不能明文展示\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=\"https://api.aaaapi.com/v1\"\n",
    "        )\n",
    "# 创建一个简单的聊天完成请求\n",
    "user_prompt = \"你好,请介绍一下你自己\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    ")\n",
    "# 返回助手的回复\n",
    "assistant_reply = response.choices[0].message.content\n",
    "\n",
    "\n",
    "# 测试代码\n",
    "print(f\"用户: {user_prompt}\")\n",
    "print(f\"AI助手: {assistant_reply}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加系统提示词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"你是一个谜语人，总是用谜语来对话。谜底必须对应用户的答案\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},  # 系统提示词是第一个加载的\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    ")\n",
    "# 返回助手的回复\n",
    "assistant_reply = response.choices[0].message.content\n",
    "\n",
    "\n",
    "# 测试代码\n",
    "print(f\"用户: {user_prompt}\")\n",
    "print(f\"AI助手: {assistant_reply}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加对话上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},  # 系统提示词是第一个加载的\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"我是一本无字之书，读来似懂非懂。无需打开，便在手中展现玄机。这书名是啥？\"},\n",
    "        {\"role\": \"user\", \"content\": \"难道你是灯泡？\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"用户: 难道你是天书？\")\n",
    "print(f\"AI助手: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提问，当我们说Prompt Engineering时，我们说的是什么？\n",
    "- A: System Prompt\n",
    "- B: 1st User Prompt\n",
    "- C: All User Prompt\n",
    "- D: Chat History?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 提高大模型输出质量的四种方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Copilot\n",
    "\n",
    "充分发挥大模型横向联想的能力。LLM eval其实很少做One attempt。\n",
    "大模型的Token是极其便宜的，联想上的性价比爆击人类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(user_prompt:str, model_name:str=\"gpt-4o\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,  \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response_with_token(user_prompt:str, model_name:str=\"gpt-4o\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,  \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content, response.usage.prompt_tokens, response.usage.completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, token_input, token_output = get_llm_response_with_token(\"请写3首藏头诗。主题是'昨天风好大'\")\n",
    "print(f\"Token input: {token_input}, Token output: {token_output}, Cost is {token_input * 10/1000000*7.26 + token_output * 30/1000000*7.26} RMB\")\n",
    "display(print(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(print(get_llm_response(\"请写3首藏头诗。主题是'昨天风好大'\", model_name=\"claude-3-5-sonnet-20240620\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(print(get_llm_response(\"请写3首藏头诗。主题是'昨天风好大'\", model_name=\"moonshot-v1-8k\")))  # KIMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 In Context Learning\n",
    "\n",
    "给大模型提供例子是让大模型“懂你”最快的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt_1 = \"\"\"请给我写一个拜年微信消息，模仿\n",
    "```\n",
    "我决定再也不发疯狂星期四文案了，疯狂星期四是五毒之首，是洪水猛兽，是离间我和朋友感情的元凶，是信任消失的的罪魁祸首，是纯情少年的无情杀手，疯狂星期四千万碰不得，疯狂星期四万万摸不得，每周四发疯四文案只能坏了大事，同意我的v我50作为封口费。\n",
    "```\n",
    "\"\"\"\n",
    "display(print(get_llm_response(context_prompt_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt_2 = \"\"\"请给我写一个拜年微信消息，模仿\n",
    "```\n",
    "To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them. \n",
    "```\n",
    "\"\"\"\n",
    "display(print(get_llm_response(context_prompt_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"9.9和9.11哪个大\"\n",
    "display(print(get_llm_response(prompt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"9.9和9.11哪个大。 Please think step by step.\"\n",
    "display(print(get_llm_response(prompt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 提示词模版\n",
    "\n",
    "事实上提示词模版并不能保证提高大模型输出质量。在GPT 3.5年代，高度依赖提示词的结构化；在GPT 4年代，提示词模版的性价比降低了很多。\n",
    "\n",
    "但是如果你要做一款大模型产品，提示词模版依然是必须掌握的技能。因为很有可能你的场景无法进行多轮对话，或者你不能指望你用户的提示词水平"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三元素提示词模版\n",
    "\n",
    "作为<角色>，你的目标是<目标>,你的任务是<任务>。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"作为一个数学家，你的目标是仔细推理，确保答案正确。你的任务是比较9.9和9.11哪个大。\"\n",
    "display(print(get_llm_response(prompt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COSTAR - 话术用\n",
    "```\n",
    "# 背景（Context）\n",
    "# 目标（Objective）\n",
    "# 风格（Style）\n",
    "# 调性（Tone）\n",
    "# 受众（Audience）\n",
    "# 格式（Response）\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costar_prompt = \"\"\"# 背景（Context）\n",
    "教师节来临，要祝福教师同事节日快乐\n",
    "# 目标（Objective）\n",
    "1. 表扬教师爱岗敬业的精神\n",
    "2. 激发荣誉感以鼓励其继续为学生服务\n",
    "# 风格（Style）\n",
    "婉约派诗词，类似柳永\n",
    "# 调性（Tone）\n",
    "温柔含蓄\n",
    "# 受众（Audience）\n",
    "25-35岁的高知女性为主\n",
    "# 格式（Response）\n",
    "词牌名：雨霖铃\"\"\"\n",
    "\n",
    "display(print(get_llm_response(costar_prompt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KIMI - 推理用\n",
    "```\n",
    "# 角色（Role）\n",
    "# 背景（Background）\n",
    "# 画像（Profile）\n",
    "# 技能（Skills）\n",
    "# 目标（Goals）\n",
    "# 限制（Constraints）\n",
    "# 工作流程（Workflow）\n",
    "# 例子（Examples）\n",
    "# 输出格式（Output Format）\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now You Try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "阅读下面的材料，根据要求写作。\n",
    "\n",
    "    随着互联网的普及、人工智能的应用，越来越多的问题能很快得到答案。那么，我们的问题是否会越来越少？\n",
    "\n",
    "以上材料引发了你怎样的联想和思考？请写一篇文章。\n",
    "要求：选准角度，确定立意，明确文体，自拟标题；不要套作，不得抄袭；不得泄露个人信息"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generation-ai-pKB8m0do-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
