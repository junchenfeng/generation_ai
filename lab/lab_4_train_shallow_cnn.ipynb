{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4:A Glimpse of Deep Learning\n",
    "\n",
    "这个Lab利用MNIST数据集来给大家展示卷积神经网络（CNN）的训练过程，以及如何对于CNN模型进行调优。\n",
    "同时，通过比较CPU和GPU的训练时间，让大家体会算力在Deep Learning中的作用\n",
    "\n",
    "*NOTE*：如果你不使用Colab，就需要提前预装TensorFlow。\n",
    "\n",
    "如果在Mac上，可以使用这个项目里的poetry，\n",
    "```\n",
    "poetry install\n",
    "```\n",
    "\n",
    "如果在Windows上，你需要自己探索（可以找大模型帮忙）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这个脚本告诉你，你是否可以使用GPU进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  details = tf.config.experimental.get_device_details(gpus[0])\n",
    "  print(\"GPU details: \", details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据加载和预处理\n",
    "\n",
    "Tensor通用shape是(N, H, W, C)，其中N是样本数量，H是高度，W是宽度，C是通道数。\n",
    "在MNIST数据集中，每个图像是28x28的灰度图，所以通道数C=1。\n",
    "\n",
    "经验来看，图像数据通常将数据归一化到0-1之间，这样有利于模型的训练。\n",
    "\n",
    "在Keras中，标签通常使用One-Hot编码，这样有利于计算分类的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始数据集的图像\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_train[i].reshape(28,28), cmap='gray')\n",
    "    plt.title(np.argmax(y_train[i]))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据归一化\n",
    "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# 标签One-Hot编码\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW**：为什么要除255来归一化？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 一个最简单的CNN\n",
    "\n",
    "下面这个模型是一个最简单的CNN模型\n",
    "\n",
    "    - “特征工程”：卷积层 + 池化层 + 展平层\n",
    "    - “分类器”：全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model = Sequential()\n",
    "\n",
    "# 添加卷积层\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "\n",
    "# 添加池化层\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 展平层\n",
    "model.add(Flatten())\n",
    "\n",
    "# 全连接层\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# 输出层， 10 对应10个类别\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO*：\n",
    "\n",
    "- 卷积层和池化层的作用是什么？\n",
    "- 卷积层的kernel_size是什么意思？\n",
    "- 池化层的pool_size是什么意思？\n",
    "- 卷积层的activation是什么？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置训练参数，可以类比ML中的模型参数\n",
    "- optimizer：常见的有SGD和Adam\n",
    "- loss：常见的有categorical_crossentropy和MSE\n",
    "- metrics：常见的有accuracy和f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认使用CPU进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "validation_data=(X_test, y_test), \n",
    "epochs=5, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你在Colab上，可以尝试使用GPU进行训练，需要修改runtime/运行时的机器类别到GPU\n",
    "\n",
    "如果你在Mac上，上面的检测脚本告诉你有GPU，那么你可以使用GPU进行训练，看看相比CPU节约了多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):  \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较训练数据集和验证数据集的准确率，来观察是否需要调整epochs，是否有过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制准确率曲线\n",
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='validation_accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制损失曲线\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**：如果过拟合了，train loss/validation loss有会有什么模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LeNet-5\n",
    "\n",
    "LeNet-5是CNN的经典模型，在1998年由Yann LeCun提出，用于手写数字的识别。\n",
    "\n",
    "**TODO**：比较LeNet-5和上面的最简单的CNN模型，有哪些区别\n",
    "\n",
    "Deep Learning的模型构建就像搭乐高积木一样，通过不同的积木组合，来测试是否有更好的效果。\n",
    "\n",
    "除了上面的这三种积木，在序列模型中还有RNN，Transformer，LSTM，GRU等积木。有兴趣的同学可以找到相关的paper，用大模型辅助你去复现一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "lenet5 = Sequential()\n",
    "\n",
    "# 第一个卷积层和池化层\n",
    "lenet5.add(Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(28,28,1), padding='same'))\n",
    "lenet5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 第二个卷积层和池化层\n",
    "lenet5.add(Conv2D(16, kernel_size=(5, 5), activation='tanh'))\n",
    "lenet5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 展平层\n",
    "lenet5.add(Flatten())\n",
    "\n",
    "# 全连接层\n",
    "lenet5.add(Dense(120, activation='tanh'))\n",
    "lenet5.add(Dense(84, activation='tanh'))\n",
    "\n",
    "# 输出层\n",
    "lenet5.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用SGD优化器\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "lenet5.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_lenet5 = lenet5.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW**：读了这张图，你会如何决策下一步训练？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制准确率曲线\n",
    "plt.plot(history_lenet5.history['loss'], label='train_loss')\n",
    "plt.plot(history_lenet5.history['val_loss'], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = lenet5.evaluate(X_test, y_test)\n",
    "print(f'LeNet-5 Test Loss: {loss}')\n",
    "print(f'LeNet-5 Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 炼一炉丹\n",
    "\n",
    "和ML选型相对还有一些数学直觉和验证之外，DL的参数选型的反馈只有通过数据集的验证才能看到结果。这种方式非常接近中世纪化学发明前的Alchemy，所以DL也被称为“炼金术”，DL研究人员戏称自己为“炼丹师”。\n",
    "\n",
    "你可以挑选一些参数，来观察他们的变化对模型训练的影响。\n",
    "\n",
    "    - 模型架构：增加卷积层，改变activation方式，改变kernel size和stride\n",
    "    - 优化器：SGD，Adam[基本是标配]，AdamW；学习速率\n",
    "    - 损失函数：categorical_crossentropy[基本是标配]，MSE\n",
    "    - 训练过程：epochs，batch size\n",
    "\n",
    "**HW**：尝试选择变更一个参数，看看效果如何"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generation-ai-pKB8m0do-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
