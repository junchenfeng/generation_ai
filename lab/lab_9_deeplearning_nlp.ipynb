{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在colab上需要运行\n",
    "```\n",
    "!pip install -q transformers\n",
    "!pip install -q tf_keras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局限制\n",
    "max_words = 10000      # 仅考虑最常用的 10,000 个词\n",
    "max_len = 200          # 每条评论或截断或补长到长度 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "\n",
    "print(\"训练集大小:\", len(x_train))\n",
    "print(\"测试集大小:\", len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 经典NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 2. 构建索引-单词映射，用于将数字序列解码回文本\n",
    "word_index = imdb.get_word_index()\n",
    "# 官方文档里 0,1,2,3 分别是特殊标记，这里做一次偏移\n",
    "index_to_word = {index + 3: word for (word, index) in word_index.items()}\n",
    "index_to_word[0] = \"<PAD>\"\n",
    "index_to_word[1] = \"<START>\"\n",
    "index_to_word[2] = \"<UNK>\"\n",
    "index_to_word[3] = \"<UNUSED>\"\n",
    "\n",
    "def decode_review(sequence):\n",
    "    \"\"\"\n",
    "    将 IMDB 的数字序列解码成可读文本。\n",
    "    \"\"\"\n",
    "    return \" \".join(index_to_word.get(i, \"?\") for i in sequence)\n",
    "\n",
    "# 3. 将训练和测试数据解码为文本\n",
    "train_texts = [decode_review(seq) for seq in x_train]\n",
    "test_texts = [decode_review(seq) for seq in x_test]\n",
    "\n",
    "# 4. 使用 scikit-learn 创建一个管道：\n",
    "#    - TfidfVectorizer: 将文本转换为 TF-IDF 特征向量\n",
    "#    - MultinomialNB:   使用朴素贝叶斯进行分类\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf',   MultinomialNB())\n",
    "])\n",
    "\n",
    "# 5. 训练模型\n",
    "pipeline.fit(train_texts, y_train)\n",
    "\n",
    "# 6. 在测试集上进行评估\n",
    "accuracy = pipeline.score(test_texts, y_test)\n",
    "print(\"在测试集上的准确率:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练一个带Attention的Seq模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# 第二步：填充/截断 序列\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的示例使用了 MultiHeadAttention 层（多头自注意力），并且为了方便查看注意力权重，我们指定了 return_attention_scores=True。这样在前向传播时能将「注意力分数矩阵」一起输出。\n",
    "\n",
    "注意：这个模型非常简单，仅仅为了演示如何调用 MultiHeadAttention，实际使用时可以加上 Position Embedding、层归一化、残差连接等，使之更贴近真正的 Transformer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "embedding_dim = 32  # 词向量维度\n",
    "num_heads = 3       # MultiHeadAttention 的头数\n",
    "\n",
    "# 输入：句子长度 = max_len\n",
    "inputs = layers.Input(shape=(max_len,))\n",
    "\n",
    "# 1) Embedding 层，将 [batch_size, max_len] -> [batch_size, max_len, embedding_dim]\n",
    "x = layers.Embedding(\n",
    "    input_dim=max_words,\n",
    "    output_dim=embedding_dim,\n",
    "    input_length=max_len\n",
    ")(inputs)\n",
    "\n",
    "# 2) MultiHeadAttention\n",
    "# 为了方便获取注意力分数，这里将 return_attention_scores=True\n",
    "# 注意力输入 Q, K, V 都用同一个张量 x（自注意力）\n",
    "attention_layer = layers.MultiHeadAttention(\n",
    "    num_heads=num_heads, \n",
    "    key_dim=embedding_dim, \n",
    "    #return_attention_scores=True\n",
    ")\n",
    "\n",
    "# MultiHeadAttention 的输出是 (attn_output, attn_scores)\n",
    "attn_output, attn_scores = attention_layer(\n",
    "    query=x, \n",
    "    value=x, \n",
    "    key=x, \n",
    "    return_attention_scores=True\n",
    ")\n",
    "\n",
    "# 3) 这里为了简化，直接对 attn_output 做一个全局平均池化\n",
    "#    相当于将 [batch_size, max_len, embedding_dim] -> [batch_size, embedding_dim]\n",
    "pooled = layers.GlobalAveragePooling1D()(attn_output)\n",
    "\n",
    "# 4) 分类层\n",
    "outputs = layers.Dense(1, activation='sigmoid')(pooled)\n",
    "\n",
    "# 构建模型\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=4,     \n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"测试集准确率: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看attention机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让 \"attention_model\" 的输出变成 attn_scores\n",
    "# 相当于把上面主干网络中的 MultiHeadAttention 重新拿出来输出分数\n",
    "attention_model = Model(inputs=inputs, outputs=attn_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_input = x_train[:1]  # 取第一条训练样本做演示\n",
    "attn_scores_out = attention_model.predict(sample_input)\n",
    "\n",
    "print(\"注意力分数的形状:\", attn_scores_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 创建一个图形，包含n个子图\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(15, 4))  # 1行3列的布局，图形大小15x4\n",
    "\n",
    "# 显示前3个注意力头的矩阵\n",
    "for i in range(num_heads):\n",
    "    attention_matrix = attn_scores_out[0][i]  # shape = (200, 200)\n",
    "    im = axes[i].imshow(attention_matrix, cmap='hot', interpolation='nearest')\n",
    "    axes[i].set_title(f\"Attention Scores (Head={i})\")\n",
    "    fig.colorbar(im, ax=axes[i])\n",
    "\n",
    "plt.tight_layout()  # 自动调整子图之间的间距\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何使用预训练模型 DiBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# 1) 填充序列到固定长度 maxlen=256\n",
    "x_train_padded = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test_padded  = pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "# 2) 现在 x_train_padded, y_train 就是形状固定的 Numpy array\n",
    "print(x_train_padded.shape)  # (25000, 256)\n",
    "\n",
    "# 3) 可以直接 from_tensor_slices\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train_padded, y_train))\n",
    "test_ds  = tf.data.Dataset.from_tensor_slices((x_test_padded, y_test))\n",
    "\n",
    "# 4) 构建 “整数->文本” 的逻辑\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {v: k for (k, v) in word_index.items()}\n",
    "\n",
    "def decode_review(int_arr):\n",
    "    # 由于是定长数组，这里 int_arr 可能含有 padding\n",
    "    # 你可以只保留非零部分 (如果你把 0 当成 PAD)\n",
    "    # 这里演示一下简化版写法\n",
    "    return \" \".join([reverse_word_index.get(i - 3, \"?\") for i in int_arr if i >= 3])\n",
    "\n",
    "def to_text_fn(int_seq, label):\n",
    "    text_tensor = tf.py_function(\n",
    "        func=lambda seq: decode_review(seq.numpy()),\n",
    "        inp=[int_seq],\n",
    "        Tout=tf.string\n",
    "    )\n",
    "    return text_tensor, label\n",
    "\n",
    "train_ds_text = train_ds.map(to_text_fn)\n",
    "test_ds_text  = test_ds.map(to_text_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看第一条数据\n",
    "for text, label in train_ds_text.take(1):\n",
    "    print(\"文本:\", text.numpy().decode('utf-8'))\n",
    "    print(\"标签:\", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查是否有可用的GPU\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# 允许GPU内存动态增长，避免占用全部显存\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# 5. 准备 BERT 模型与 tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2  # 二分类\n",
    ")\n",
    "# 6. 定义给 BERT 做分词预处理的函数\n",
    "def bert_preprocess_batch(texts, labels):\n",
    "    \"\"\"\n",
    "    texts: shape=(batch_size,), dtype=string\n",
    "    labels: shape=(batch_size,), dtype=int64 (或 int32)\n",
    "    \"\"\"\n",
    "    # 先把 texts 转为 Python list[str]\n",
    "    py_texts = [t.decode(\"utf-8\") for t in texts.numpy()]\n",
    "\n",
    "    # 对这一批文本做分词\n",
    "    encoding = tokenizer(\n",
    "        py_texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "\n",
    "    return (\n",
    "        encoding[\"input_ids\"],      # (batch_size, 128)\n",
    "        encoding[\"attention_mask\"], # (batch_size, 128)\n",
    "        labels                      # (batch_size,)\n",
    "    )\n",
    "\n",
    "def wrap_preprocess_batch(texts, labels):\n",
    "    # 通过 tf.py_function 调用上面的 Python 函数\n",
    "    outputs = tf.py_function(\n",
    "        func=bert_preprocess_batch,\n",
    "        inp=[texts, labels],\n",
    "        Tout=[tf.int32, tf.int32, tf.int64]\n",
    "    )\n",
    "    # outputs 是长度为 3 的列表: [input_ids, attention_mask, labels]\n",
    "    # 这三个张量的 shape 现在还都是 (None,) 或 <unknown>\n",
    "\n",
    "    # 显式设置形状！\n",
    "    # 假设你 batch(16)，然后 truncation 后的序列长度 = 128\n",
    "    outputs[0].set_shape((None, 128))  # input_ids\n",
    "    outputs[1].set_shape((None, 128))  # attention_mask\n",
    "    outputs[2].set_shape((None,))      # label\n",
    "\n",
    "    # Keras 的模型需要 {\"input_ids\":..., \"attention_mask\":...}, label\n",
    "    return {\n",
    "        'input_ids': outputs[0],\n",
    "        'attention_mask': outputs[1]\n",
    "    }, outputs[2]\n",
    "\n",
    "\n",
    "def prepare_bert_dataset(ds, shuffle=False, batch_size=16):\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.map(wrap_preprocess_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "    return ds\n",
    "\n",
    "# 7. 构造训练和测试集\n",
    "train_ds_bert = prepare_bert_dataset(train_ds_text, shuffle=True, batch_size=16)\n",
    "test_ds_bert = prepare_bert_dataset(test_ds_text, shuffle=False, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. 编译并训练模型\n",
    "bert_model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 对BERT模型同样可以使用GPU\n",
    "with tf.device('/GPU:0'):\n",
    "    history_bert = bert_model.fit(\n",
    "        train_ds_bert,\n",
    "        validation_data=test_ds_bert,\n",
    "        epochs=1\n",
    "    )\n",
    "# 9. 评估\n",
    "test_loss_bert, test_acc_bert = bert_model.evaluate(test_ds_bert)\n",
    "print(\"Final Test Accuracy with DistilBERT:\", test_acc_bert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generation-ai-pKB8m0do-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
