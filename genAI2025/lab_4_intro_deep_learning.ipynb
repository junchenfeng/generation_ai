{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4:An Introduction of Deep Learning\n",
    "\n",
    "这个Lab利用MNIST数据集来给大家展示卷积神经网络（CNN）的训练过程，以及如何对于CNN模型进行调优。\n",
    "同时，通过比较CPU和GPU的训练时间，让大家体会算力在Deep Learning中的作用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这个脚本告诉你，你是否可以使用GPU进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  details = tf.config.experimental.get_device_details(gpus[0])\n",
    "  print(\"GPU details: \", details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 图像模型 & LeNet-5\n",
    "\n",
    "## 1.1 理解tensor的数据类型\n",
    "\n",
    "Tensor通用shape是(N, H, W, C)，其中N是样本数量，H是高度，W是宽度，C是通道数。\n",
    "在MNIST数据集中，每个图像是28x28的灰度图，所以通道数C=1。\n",
    "\n",
    "经验来看，图像数据通常将数据归一化到0-1之间，这样有利于模型的训练。\n",
    "\n",
    "在Keras中，标签通常使用One-Hot编码，这样有利于计算分类的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始数据集的图像\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_train[i].reshape(28,28), cmap='gray')\n",
    "    plt.title(np.argmax(y_train[i]))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据归一化\n",
    "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# 标签One-Hot编码\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**思考**：为什么要除255来归一化？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 一个最简单的CNN\n",
    "\n",
    "下面这个模型是一个最简单的CNN模型\n",
    "\n",
    "    - “特征工程”：卷积层 + 池化层 + 展平层\n",
    "    - “分类器”：全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model = Sequential()\n",
    "\n",
    "# 添加卷积层\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "\n",
    "# 添加池化层\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 展平层\n",
    "model.add(Flatten())\n",
    "\n",
    "# 全连接层\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# 输出层， 10 对应10个类别\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ask LLM*：\n",
    "\n",
    "- 卷积层和池化层的作用是什么？\n",
    "- 卷积层的kernel_size是什么意思？\n",
    "- 池化层的pool_size是什么意思？\n",
    "- 卷积层的activation是什么？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置训练参数，可以类比ML中的模型参数\n",
    "- optimizer：常见的有SGD和Adam\n",
    "- loss：常见的有categorical_crossentropy和MSE\n",
    "- metrics：常见的有accuracy和f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认使用CPU进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "validation_data=(X_test, y_test), \n",
    "epochs=5, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用GPU训练的方法\n",
    "\n",
    "with tf.device('/GPU:0'):  \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较训练数据集和验证数据集的准确率，来观察是否需要调整epochs，是否有过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制准确率曲线\n",
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='validation_accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制损失曲线\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**思考**：如果过拟合了，train loss/validation loss有会有什么模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. 架构升级：LeNet-5\n",
    "\n",
    "LeNet-5是CNN的经典模型，在1998年由Yann LeCun提出，用于手写数字的识别。\n",
    "\n",
    "**TODO**：比较LeNet-5和上面的最简单的CNN模型，有哪些区别\n",
    "\n",
    "Deep Learning的模型构建就像搭乐高积木一样，通过不同的积木组合，来测试是否有更好的效果。\n",
    "\n",
    "除了上面的这三种积木，在序列模型中还有RNN，Transformer，LSTM，GRU等积木。有兴趣的同学可以找到相关的paper，用大模型辅助你去复现一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "lenet5 = Sequential()\n",
    "\n",
    "# 第一个卷积层和池化层\n",
    "lenet5.add(Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(28,28,1), padding='same'))\n",
    "lenet5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 第二个卷积层和池化层\n",
    "lenet5.add(Conv2D(16, kernel_size=(5, 5), activation='tanh'))\n",
    "lenet5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 展平层\n",
    "lenet5.add(Flatten())\n",
    "\n",
    "# 全连接层\n",
    "lenet5.add(Dense(120, activation='tanh'))\n",
    "lenet5.add(Dense(84, activation='tanh'))\n",
    "\n",
    "# 输出层\n",
    "lenet5.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用SGD优化器\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "lenet5.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_lenet5 = lenet5.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**思考**：读了这张图，你会如何决策下一步训练？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制准确率曲线\n",
    "plt.plot(history_lenet5.history['loss'], label='train_loss')\n",
    "plt.plot(history_lenet5.history['val_loss'], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = lenet5.evaluate(X_test, y_test)\n",
    "print(f'LeNet-5 Test Loss: {loss}')\n",
    "print(f'LeNet-5 Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NLP & Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['Heiti TC']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 文本的序列化\n",
    "\n",
    "- 词元 / Token\n",
    "- 嵌入向量 / Embedding\n",
    "    + 捕捉词元的意义及其与其他词元的关系\n",
    "    +  [Projector](https://projector.tensorflow.org/)\n",
    "    + 嵌入向量的数学计算带有语义含义\n",
    "        -  King - Male + Female = Queen\n",
    "- 位置编码 / Positional Encoding\n",
    "    + “猫追狗” VS “狗追猫”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 数据加载和预处理\n",
    "print(\"=== 步骤1: 数据加载 ===\")\n",
    "\n",
    "# 加载IMDB数据集，只使用前5000个最常见的词\n",
    "vocab_size = 5000\n",
    "max_length = 100  # 限制序列长度以便可视化\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "print(f\"训练集大小: {len(x_train)}\")\n",
    "print(f\"测试集大小: {len(x_test)}\")\n",
    "print(f\"第一个样本原始长度: {len(x_train[0])}\")\n",
    "print(f\"第一个样本前10个token: {x_train[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 展示Token化过程\n",
    "print(\"=== 步骤2: Token化演示 ===\")\n",
    "\n",
    "# 获取词汇表映射\n",
    "word_index = imdb.get_word_index()\n",
    "# 创建反向映射（从索引到词）\n",
    "reverse_word_index = {v+3: k for k, v in word_index.items()}\n",
    "reverse_word_index[0] = '<PAD>'\n",
    "reverse_word_index[1] = '<START>'\n",
    "reverse_word_index[2] = '<UNK>'\n",
    "reverse_word_index[3] = '<UNUSED>'\n",
    "\n",
    "def decode_review(encoded_review):\n",
    "    \"\"\"将token序列转换回文本\"\"\"\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in encoded_review])\n",
    "\n",
    "# 展示原始文本和token化结果\n",
    "sample_idx = 0\n",
    "print(f\"样本{sample_idx}的标签: {'正面' if y_train[sample_idx] == 1 else '负面'}\")\n",
    "print(f\"原始token序列前20个: {x_train[sample_idx][:20]}\")\n",
    "print(f\"对应的文本: {decode_review(x_train[sample_idx][:20])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transformer架构\n",
    "- 自注意力机制（self-attention）：保持长距离记忆\n",
    "    + 把面粉、鸡蛋和牛奶放进搅拌机,将*它*摇匀\n",
    "- 查询、键和值（**Q**ueries, **K**ey, **V**alue）\n",
    "    + 查询：它\n",
    "    + 键：面粉、鸡蛋、牛奶，放进，搅拌机，均匀\n",
    "    + 值：代表查询/键关系强弱的数字\n",
    "- Attention\n",
    "    + $$\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "    + 如果你真的好奇原理，推荐[Jay Alammar](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "- Encoder / Decoder\n",
    "    + Encoder：Self-Attention + Feedfoward\n",
    "    + Decoder：Self-Attention + Encoder-Decoder Attention + Feedward\n",
    "    + 将encoder和decoder堆叠起来，形成transfomrer；再把transfomer堆叠起来，变成模型架构\n",
    "\n",
    "- 同构异参：训练的关键是数据量（数量和质量）以及模型参数数量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 序列填充和准备\n",
    "print(\"=== 步骤3: 序列填充 ===\")\n",
    "\n",
    "# 填充序列到固定长度\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "# 标签转换为二分类\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# 4. 简单的Transformer模型定义\n",
    "print(\"=== 步骤4: Transformer模型构建 ===\")\n",
    "\n",
    "class SimpleAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads=1):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = layers.Dense(d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        \"\"\"计算注意力权重\"\"\"\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        \n",
    "        # 缩放\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        \n",
    "        # 添加mask（可选）\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "            \n",
    "        # softmax\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        \n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "        \n",
    "        # 计算注意力\n",
    "        attention, attention_weights = self.scaled_dot_product_attention(q, k, v)\n",
    "        \n",
    "        output = self.dense(attention)\n",
    "        \n",
    "        # 保存注意力权重用于可视化\n",
    "        self.last_attention_weights = attention_weights\n",
    "        \n",
    "        return output\n",
    "\n",
    "def create_simple_transformer(vocab_size, max_length, embedding_dim=64):\n",
    "    \"\"\"创建简单的Transformer模型\"\"\"\n",
    "    inputs = layers.Input(shape=(max_length,))\n",
    "    \n",
    "    # Embedding层\n",
    "    embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs)\n",
    "    \n",
    "    # 位置编码（简化版）\n",
    "    positions = tf.range(start=0, limit=max_length, delta=1)\n",
    "    positions = layers.Embedding(max_length, embedding_dim)(positions)\n",
    "    x = embedding + positions\n",
    "    \n",
    "    # Transformer块\n",
    "    attention = SimpleAttention(embedding_dim)\n",
    "    attended = attention(x)\n",
    "    \n",
    "    # 残差连接和层归一化\n",
    "    x = layers.Add()([x, attended])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    \n",
    "    # 前馈网络\n",
    "    ff = layers.Dense(128, activation='relu')(x)\n",
    "    ff = layers.Dense(embedding_dim)(ff)\n",
    "    \n",
    "    # 残差连接和层归一化\n",
    "    x = layers.Add()([x, ff])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    \n",
    "    # 全局平均池化\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # 分类层\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    return model, attention\n",
    "\n",
    "# 创建模型\n",
    "embedding_dim = 64\n",
    "model, attention_layer = create_simple_transformer(vocab_size, max_length, embedding_dim)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 训练模型（简短训练用于演示）\n",
    "print(\"=== 步骤6: 模型训练 ===\")\n",
    "\n",
    "# 使用小批量数据进行快速演示\n",
    "train_samples = 1000\n",
    "test_samples = 200\n",
    "\n",
    "history = model.fit(\n",
    "    x_train[:train_samples], \n",
    "    y_train[:train_samples],\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    validation_data=(x_test[:test_samples], y_test[:test_samples]),\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 可视化注意力矩阵\n",
    "print(\"=== 步骤7: 注意力机制可视化 ===\")\n",
    "\n",
    "# 获取一个样本的注意力权重\n",
    "sample_for_attention = x_train[sample_idx:sample_idx+1]\n",
    "_ = model(sample_for_attention)  # 前向传播以计算注意力\n",
    "\n",
    "# 获取注意力权重 - 修复：检查是否已经是numpy数组\n",
    "attention_weights = attention_layer.last_attention_weights[0]\n",
    "if hasattr(attention_weights, 'numpy'):\n",
    "    attention_weights = attention_weights.numpy()  # 如果是tensor则转换\n",
    "# 如果已经是numpy数组，直接使用\n",
    "\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")\n",
    "\n",
    "# 只显示前20x20的注意力矩阵以便观察\n",
    "seq_len_to_show = min(20, attention_weights.shape[0])\n",
    "attention_subset = attention_weights[:seq_len_to_show, :seq_len_to_show]\n",
    "tokens_subset = sample_for_attention[0, :seq_len_to_show]\n",
    "\n",
    "# 确保tokens_subset也是numpy数组\n",
    "if hasattr(tokens_subset, 'numpy'):\n",
    "    tokens_subset = tokens_subset.numpy()\n",
    "\n",
    "words_subset = [reverse_word_index.get(int(token), '<PAD>') for token in tokens_subset]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    attention_subset,\n",
    "    xticklabels=words_subset,\n",
    "    yticklabels=words_subset,\n",
    "    cmap='Blues',\n",
    "    annot=False,\n",
    "    fmt='.2f'\n",
    ")\n",
    "plt.title('注意力权重矩阵 (前20个token)')\n",
    "plt.xlabel('Key (被关注的词)')\n",
    "plt.ylabel('Query (查询的词)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 显示具体的注意力值\n",
    "print(\"\\n注意力权重解释:\")\n",
    "print(\"矩阵中的每一行表示一个query词对所有key词的注意力权重\")\n",
    "print(\"颜色越深表示注意力权重越高\")\n",
    "print(f\"样本文本: {decode_review(tokens_subset)}\")\n",
    "\n",
    "# 显示一些具体的注意力值示例\n",
    "print(f\"\\n示例：第一个词 '{words_subset[0]}' 对其他词的注意力权重:\")\n",
    "for j in range(min(5, len(words_subset))):\n",
    "    print(f\"  对 '{words_subset[j]}' 的注意力: {attention_subset[0, j]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 模型预测和解释\n",
    "print(\"=== 步骤8: 模型预测演示 ===\")\n",
    "\n",
    "# 预测几个样本\n",
    "test_samples_for_pred = x_test[:5]\n",
    "predictions = model.predict(test_samples_for_pred)\n",
    "\n",
    "for i in range(5):\n",
    "    actual_label = \"正面\" if y_test[i] == 1 else \"负面\"\n",
    "    predicted_prob = predictions[i][0]\n",
    "    predicted_label = \"正面\" if predicted_prob > 0.5 else \"负面\"\n",
    "    \n",
    "    print(f\"\\n样本 {i+1}:\")\n",
    "    print(f\"文本片段: {decode_review(test_samples_for_pred[i][:30])}\")\n",
    "    print(f\"实际标签: {actual_label}\")\n",
    "    print(f\"预测概率: {predicted_prob:.4f}\")\n",
    "    print(f\"预测标签: {predicted_label}\")\n",
    "    print(f\"预测{'正确' if actual_label == predicted_label else '错误'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Decoder Only & GPT\n",
    "\n",
    "- Decoder Only架构：只看上文，不看下文 -> next token prediction\n",
    "- 在参数量极大的情况下依然能够保留长距离记忆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回家作业【选做】：以MNIST炼一炉丹\n",
    "\n",
    "和ML选型相对还有一些数学直觉和验证之外，DL的参数选型的反馈只有通过数据集的验证才能看到结果。\n",
    "\n",
    "这种方式非常接近中世纪化学发明前的Alchemy，所以DL也被称为“炼金术”，DL研究人员戏称自己为“炼丹师”。\n",
    "\n",
    "你可以挑选一些参数，来观察他们的变化对模型训练的影响。\n",
    "\n",
    "    - 模型架构：增加卷积层，改变activation方式，改变kernel size和stride\n",
    "    - 优化器：SGD，Adam[基本是标配]，AdamW；学习速率\n",
    "    - 损失函数：categorical_crossentropy[基本是标配]，MSE\n",
    "    - 训练过程：epochs，batch size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai2025-EbBYtNEX-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
